# 交叉熵
交叉熵是信息论与机器学习中的核心概念，核心作用是衡量两个概率分布的差异。

## 核心定义
交叉熵是描述两个概率分布 P（真实分布）和 Q（模型预测分布）之间“不匹配程度”的指标。
本质是用真实分布的权重，衡量预测分布的不确定性代价。

## 主要用途
1. 机器学习损失函数：分类任务中最常用，比如逻辑回归、神经网络，通过最小化交叉熵让模型预测分布逼近真实标签分布。
2. 衡量概率分布相似度：交叉熵越小，两个分布越接近，可用于比较不同模型的预测效果。
3. 信息论基础工具：用于计算相对熵（KL散度），KL散度 = 交叉熵 - 熵，是模型优化的核心依据。

## 类型
### 1. 离散型交叉熵（最常用）
适用于标签为离散类别（包括二分类和多分类务）的场景，是机器学习中最常接触的形式。
- 公式：H(P,Q) = -Σ[P(xᵢ) × logQ(xᵢ)]
- 变量说明：P(xᵢ)是真实分布在第i个类别上的概率（如标签为1时P=1，其他为0），Q(xᵢ)是模型预测的第i个类别概率，求和覆盖所有可能的类别。
- 简化场景（二分类）：H(P,Q) = -[y×logp + (1-y)×log(1-p)]，其中y是真实标签（0或1），p是模型预测为正类的概率。

### 2. 连续型交叉熵
适用于概率分布为连续的场景（如概率密度函数），需通过积分计算。
- 公式：H(P,Q) = -∫P(x)×logQ(x)dx
- 变量说明：P(x)和Q(x)分别是真实和预测的概率密度函数，积分范围覆盖x的所有可能取值。

**特殊说明**
上述公式默认以2为底（信息论中衡量比特数），机器学习中常改用自然对数（e为底），形式不变仅底数不同：

