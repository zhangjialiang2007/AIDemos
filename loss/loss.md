损失函数是机器学习中衡量模型预测与真实标签差异的核心指标，不同任务（如分类、回归、生成等）需要选择不同的损失函数。除了交叉熵（常用于分类），常见的损失函数还包括以下几类：


### **一、回归任务常用损失函数**
回归任务的目标是预测连续值（如房价、温度），损失函数衡量预测值与真实值的“距离”。

#### 1. 均方误差（Mean Squared Error, MSE）
- **公式**：$L = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2$  
  其中$y_i$是真实值，$\hat{y}_i$是预测值，$n$是样本数。
- **特点**：对 outliers（异常值）敏感（平方会放大误差），适用于数据分布较均匀的场景。
- **用途**：线性回归、神经网络回归任务等。

#### 2. 平均绝对误差（Mean Absolute Error, MAE）
- **公式**：$L = \frac{1}{n} \sum_{i=1}^n |y_i - \hat{y}_i|$  
- **特点**：对异常值更稳健（误差不会被平方放大），但梯度在0点不连续（可能影响优化效率）。
- **用途**：房价预测、销量预测等对异常值敏感的场景。

#### 3. 均方根误差（Root Mean Squared Error, RMSE）
- **公式**：$L = \sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2}$  
- **特点**：MSE的平方根，量纲与目标值一致（更易解释），同样对异常值敏感。
- **用途**：需保持原始数据单位的回归任务（如身高预测，单位为cm）。

#### 4. Huber损失（平滑L1损失）
- **公式**：  
  $L = \begin{cases} 
  \frac{1}{2}(y - \hat{y})^2 & \text{if } |y - \hat{y}| \leq \delta \\
  \delta(|y - \hat{y}| - \frac{1}{2}\delta) & \text{otherwise}
  \end{cases}$  
  其中$\delta$是超参数（通常取1.0）。
- **特点**：结合MSE和MAE的优点，误差较小时用MSE（梯度平滑），误差较大时用MAE（抗异常值）。
- **用途**：目标检测中的边界框回归（如Faster R-CNN）。


### **二、分类任务常用其他损失函数**
#### 1. 0-1损失（0-1 Loss）
- **公式**：$L = \begin{cases} 0 & \text{if } \hat{y} = y \\ 1 & \text{otherwise} \end{cases}$  
- **特点**：直接衡量分类是否正确，但不连续、不可导（无法用于梯度下降优化）。
- **用途**：作为理论上的“理想损失”，实际中很少直接使用。

#### 2.  hinge损失（Hinge Loss）
- **公式**（二分类）：$L = \max(0, 1 - y \cdot \hat{y})$  
  其中$y \in \{-1, 1\}$是真实标签，$\hat{y}$是模型输出的分数（未归一化）。
- **特点**：对正确分类且置信度高的样本（$y \cdot \hat{y} \geq 1$）损失为0，专注于优化难分样本，抗噪性强。
- **用途**：支持向量机（SVM）的标准损失函数。

#### 3. 平方 hinge损失（Squared Hinge Loss）
- **公式**：$L = \max(0, 1 - y \cdot \hat{y})^2$  
- **特点**：比hinge损失更“惩罚”错误分类（平方放大误差），收敛更快但对异常值更敏感。

#### 4.  focal损失（Focal Loss）
- **公式**（二分类）：$L = -\alpha y \log(p) - (1-\alpha)(1-y)\log(1-p) \cdot (1-p)^\gamma$  
  其中$\alpha$平衡类别不平衡，$\gamma \geq 0$降低易分样本的权重（聚焦难分样本）。
- **特点**：解决类别不平衡问题（如小目标检测中少数类样本难识别），通过降低简单样本的损失占比，让模型更关注难分样本。
- **用途**：目标检测（如RetinaNet）。

### 5. 交叉熵损失
以下是补充了**交叉熵损失函数**的完整整理，包含分类任务中最常用的交叉熵（二分类与多分类），并保持与前文一致的风格：
- **公式**（二分类）：$L = -\frac{1}{n} \sum_{i=1}^n \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right]$   其中：
  - $y_i \in \{0, 1\}$ 是真实标签（二分类），  
  - $\hat{y}_i \in [0, 1]$ 是模型通过sigmoid输出的预测概率，  
  - $n$ 是样本数。
- **特点**：交叉熵本质上衡量了“用模型预测分布编码真实分布所需的平均比特数”，值越小表示两个分布越接近。在分类任务中，它通过直接优化概率分布差异，比MSE等损失函数更适合（MSE会导致梯度消失，尤其在使用sigmoid/softmax时）。
- **类型**： 二分类，多分类，稀疏多分类，连续

### **三、其他特殊任务损失函数**
#### 1. 余弦相似度损失（Cosine Similarity Loss）
- **公式**：$L = 1 - \frac{\hat{y} \cdot y}{||\hat{y}|| \cdot ||y||}$  
- **特点**：衡量两个向量的方向差异（而非大小），适用于关注“相似性”的场景。
- **用途**：人脸识别、嵌入学习（如Word2Vec）。

#### 2. KL散度（Kullback-Leibler Divergence）
- **公式**：$D_{KL}(P||Q) = \sum P(x) \log\left(\frac{P(x)}{Q(x)}\right) = H(P,Q) - H(P)$  
  其中$H(P,Q)$是交叉熵，$H(P)$是$P$的熵。
- **特点**：衡量两个概率分布的差异（非对称），常作为生成模型（如VAE）的损失项。
- **用途**：变分自编码器（VAE）、模型蒸馏。

#### 3. 对比损失（Contrastive Loss）
- **公式**：$L = \begin{cases} \frac{1}{2}(||a - b||^2) & \text{if 相似样本} \\ \frac{1}{2}\max(0, m - ||a - b||)^2 & \text{if 不相似样本} \end{cases}$  
  其中$a,b$是样本对的嵌入向量，$m$是阈值。
- **特点**：让相似样本的嵌入距离更近，不相似样本的距离大于阈值$m$。
- **用途**：孪生网络（Siamese Network）、人脸识别、签名验证。


### **总结**
选择损失函数的核心原则：
- 回归任务：优先MSE（平滑）、MAE（抗异常）或Huber（折中）。
- 分类任务：交叉熵（通用）、hinge（SVM）、focal（类别不平衡）。
- 特殊场景：余弦损失（相似度）、KL散度（分布匹配）、对比损失（样本对学习）。

需要某类损失函数的具体代码实现（如JS或Python）吗？