KL 散度用于衡量两个概率分布之间的差异

 **KL 散度公式**
 $D_{KL}(P||Q) = \sum P(i) \log\left(\frac{P(i)}{Q(i)}\right)$，其中 $P$ 是真实分布，$Q$ 是预测分布
2. **数值稳定性处理**：
   - 通过 `epsilon` 参数避免 $Q(i)=0$ 时出现 $\log(0)$ 的问题
   - 当 $P(i)=0$ 时，该项贡献为 0（符合数学定义）
3. **输入验证**：检查了输入数组长度一致性和概率值非负性下面是使用 JavaScript 实现 KL 散度（Kullback-Leibler Divergence）损失函数的代码。KL 散度用于衡量两个概率分布之间的差异，在机器学习中常被用作损失函数：

### 注意事项：
- KL 散度不是对称的，即 $D_{KL}(P||Q) \neq D_{KL}(Q||P)$
- 输入的数组需要满足概率分布的基本性质（元素非负，总和为 1）
- 在实际应用中（如神经网络），通常会配合 softmax 函数使用，确保输出符合概率分布特性


### 注意事项：
- KL 散度不是对称的，即 $D_{KL}(P||Q) \neq D_{KL}(Q||P)$
- 输入的数组需要满足概率分布的基本性质（元素非负，总和为 1）
- 在实际应用中（如神经网络），通常会配合 softmax 函数使用，确保输出符合概率分布特性